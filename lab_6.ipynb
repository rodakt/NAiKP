{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sieci neuronowe w PyTorch\n",
    "\n",
    "### Tomasz Rodak\n",
    "\n",
    "Laboratorium 6\n",
    "\n",
    "---\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 \n",
    "\n",
    "Celem tego arkusza jest wprowadzenie do sieci neuronowych w [PyTorch](https://pytorch.org/). \n",
    "\n",
    "[**Sieć neuronowa**](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) to ogólny termin oznaczający rodzinę modeli o strukturze inspirowanej budową mózgu. Proste sieci neuronowe mają strukturę jednokierunkowego grafu: dane wejściowe przechodzą przez kolejne węzły (warstwy neuronów) wzdłuż krawędzi (połączeń neuronów). Przy przejściu przez krawędź wartość jest mnożona przez wagę krawędzi, w węźle następuje agregacja wartości z krawędzi, a następnie zastosowanie funkcji aktywacji do wyniku tej agregacji.\n",
    "\n",
    "**PyTorch** to operująca na tensorach biblioteka do głębokiego uczenia, która jest zoptymalizowana pod kątem wydajności na CPU i procesorach graficznych GPU.\n",
    "\n",
    "Poniżej przedstawimy demonstrację działania prostej sieci neuronowej przeznaczonej do wykonywania klasyfikacji:\n",
    "- utworzymy syntetyczny zbiór danych w postaci punktów w przestrzeni 2D (dzięki czemu będziemy mogli wizualizować wyniki),\n",
    "- zbudujemy sieć neuronową,\n",
    "- pokażemy jak wykonać jej trening,\n",
    "- zwizualizujemy wyniki klasyfikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Importowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Generowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5_000\n",
    "r1, r2 = 1, 2\n",
    "sd = .2\n",
    "t = np.linspace(0, 2 * np.pi, N//2)\n",
    "X = np.zeros((N, 2))\n",
    "X[:N//2, 0] = r1 * np.cos(t) + np.random.normal(0, sd, N//2)\n",
    "X[:N//2, 1] = r1 * np.sin(t) + np.random.normal(0, sd, N//2)\n",
    "X[N//2:, 0] = r2 * np.cos(t) + np.random.normal(0, sd, N//2)\n",
    "X[N//2:, 1] = r2 * np.sin(t) + np.random.normal(0, sd, N//2)\n",
    "y = np.zeros(N)\n",
    "y[N//2:] = 1\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=5, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Konwersja danych do PyTorch\n",
    "\n",
    "Aby PyTorch mógł wykonać obliczenia na danych, muszą one mieć format **tensora**. Tensory to wielowymiarowe tablice numeryczne, które są podstawowym typem danych w PyTorch. W porównaniu do tablic NumPy tensory mają dodatkowe właściwości, takie jak możliwość wykonywania obliczeń na GPU, automatyczne różniczkowanie i możliwość śledzenia gradientów.\n",
    "\n",
    "Funkcja `torch.tensor()` konwertuje tablicę NumPy na tensor PyTorch. Korzystamy ponadto z:\n",
    "- `torch.utils.data.TensorDataset` - klasa, która tworzy zbiór danych z tensorów,\n",
    "- `torch.utils.data.DataLoader` - klasa, która tworzy iteratory na zbiorach danych, dzieląc je na partie zwane batchami (*ang. batch*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(X, dtype=torch.float32)\n",
    "labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(data, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 Budowanie modelu\n",
    "\n",
    "W PyTorch najczęściej definiuje się architekturę sieci neuronowej, tworząc klasę dziedziczącą po `torch.nn.Module`. W tej klasie:\n",
    "\n",
    "- **Konstruktor `__init__`:** Inicjalizuje się tutaj wszystkie warstwy sieci, takie jak warstwy liniowe (`nn.Linear`), konwolucyjne (`nn.Conv2d`), itd.\n",
    "- **Metoda `forward`:** Określa się przebieg danych przez sieć, czyli w jaki sposób kolejne warstwy przetwarzają dane wejściowe.\n",
    "\n",
    "W podanym niżej przykładzie:\n",
    "- W metodzie `__init__` definiujemy dwie warstwy liniowe.\n",
    "- W metodzie `forward` przekazujemy dane przez warstwę wejściową, stosujemy funkcję aktywacji ReLU oraz przechodzimy przez kolejną warstwę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Klasyfikator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Klasyfikator, self).__init__()\n",
    "        # Warstwa liniowa: 2 wejścia (wymiary danych), 30 neuronów w warstwie ukrytej\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        # Warstwa liniowa: 30 neuronów w warstwie ukrytej, 10 neuronów w kolejnej warstwie\n",
    "        self.fc2 = nn.Linear(4, 2)\n",
    "        # Warstwa liniowa: 10 neuronów w warstwie ukrytej, 1 wyjście (prawdopodobieństwo klasy)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "        # Funkcja aktywacji ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        # Funkcja aktywacji Sigmoid (do klasyfikacji binarnej)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Przejście przez pierwszą warstwę liniową i zastosowanie ReLU\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # Przejście przez drugą warstwę liniową i zastosowanie ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        # Przejście przez trzecią warstwę liniową i zastosowanie Sigmoid\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzenie obiektu klasy `Klasyfikator` - jest to sieć neuronowa zbudowana zgodnie z szablonem zdefiniowanym w klasie `Klasyfikator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Klasyfikator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przepchnięcie jednej próbki przez sieć neuronową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.forward(torch.tensor([[1.0, 2.0]]))  # Przykładowe wejście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak klasyfikator sam się przedstawia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podsumowanie wykonane przez funkcję `torchinfo.summary()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(clf, input_size=(1, 2), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczenie przewidywań dla wszystkich próbek w zbiorze treningowym (jeszcze przed rozpoczęciem treningu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.eval() # Ustawienie modelu w tryb ewaluacji\n",
    "y_proba = clf.forward(train_dataset[:][0])\n",
    "y_pred = y_proba > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmienna `y_proba` to tensor zawierający prawdopodobieństwa przynależności do klas. My mamy tylko dwie klasy a warstwie wyjściowej ustawiliśmy funkcję sigmoid na jednowymiarowym wyjściu, więc `y_proba` to tensor jednowymiarowy z prawdopodobieństwami przynależności do klasy 1. Tensor `y_pred` zamienia prawdopodobieństwa na klasy (0 lub 1) względem progu 0.5. \n",
    "\n",
    "Aktualna ocena modelu na zbiorze treningowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred.detach().numpy())  # Obliczenie dokładności modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_pred.detach().numpy())  # Macierz pomyłek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja przewidywań przed rozpoczęciem treningu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred.detach().numpy(), s=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Funkcja straty\n",
    "\n",
    "Funkcja straty to miara różnicy między przewidywaniami modelu a rzeczywistymi wartościami. W przypadku klasyfikacji binarnej najczęściej stosuje się funkcję straty `Binary Cross-Entropy Loss`, która jest dostępna w PyTorch jako `torch.nn.BCELoss`. Funkcja ta oblicza stratę na podstawie prawdopodobieństw przynależności do klasy 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartość straty przed treningiem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(y_proba, train_dataset[:][1].unsqueeze(1))  # Obliczenie straty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 Optymalizator\n",
    "\n",
    "Optymalizator to algorytm, który aktualizuje wagi modelu w celu minimalizacji funkcji straty. W PyTorch dostępnych jest wiele optymalizatorów, takich jak `SGD`, `Adam`, `RMSprop`, itd. W naszym przypadku użyjemy optymalizatora `Adam`, który jest dostępny w PyTorch jako `torch.optim.Adam`. Parametr `lr` to współczynnik uczenia, który kontroluje szybkość aktualizacji wag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.8 Trening modelu\n",
    "\n",
    "Trening modelu polega na wielokrotnym przechodzeniu przez zbiór danych, obliczaniu straty i aktualizowaniu wag modelu. **Iteracją** nazywamy przetworzenie jednego batcha danych - następuje wtedy obliczenie straty i aktualizacja wag. **Epoką** nazywamy przetworzenie całego zbioru danych, czyli przejście przez wszystkie batche, na które dzielimy zbiór danych. Ważna uwaga: po każdej epoce zbiór danych jest mieszany, dlatego odpowiadające sobie batch'e w kolejnych epokach zawierają różne próbki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4  # Liczba epok treningowych\n",
    "\n",
    "clf.train()  # Ustawienie modelu w tryb treningu\n",
    "\n",
    "# Pętla treningowa\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:  # Iteracja po batchach danych\n",
    "        optimizer.zero_grad()  # Wyzerowanie gradientów\n",
    "        y_proba = clf(x_batch)  # Obliczenie przewidywań modelu\n",
    "        loss = loss_fn(y_proba.flatten(), y_batch)  # Obliczenie straty\n",
    "        loss.backward()  # Obliczenie gradientów\n",
    "        optimizer.step()  # Aktualizacja wag modelu\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")  # Wyświetlenie straty z ostaniej iteracji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przewidywania modelu na zbiorze treningowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.eval()\n",
    "y_proba = clf.forward(train_dataset[:][0])\n",
    "y_pred = y_proba > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred.detach().numpy())  # Obliczenie dokładności modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_pred.detach().numpy())  # Macierz pomyłek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred.detach().numpy(), s=5, alpha=0.5, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Zadanie\n",
    "\n",
    "Wykonaj eksperymenty zmieniając:\n",
    "- liczbę neuronów w warstwie ukrytej,\n",
    "- liczbę warstw ukrytych,\n",
    "- funkcję aktywacji,\n",
    "- współczynnik uczenia,\n",
    "- liczbę epok,\n",
    "- rozmiar batcha,\n",
    "- rozmiar zbioru treningowego,\n",
    "- postać zbioru treningowego (np. zmień odchylenie standardowe `sd` generowanych punktów)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
