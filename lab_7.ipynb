{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Przetwarzanie obrazów. Sieci konwolucyjne\n",
    "\n",
    "### Tomasz Rodak\n",
    "\n",
    "Laboratorium 7\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 \n",
    "\n",
    "[*Konwolucyjne sieci neuronowe*](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) to sieci neuronowe typu feedforward, które rozpoznają cechy lokalne w danych wejściowych poprzez optymalizację *filtrów konwolucyjnych*. Sieci te można stosować do różnych typów danych, ale najczęściej wykorzystuje się je do rozpoznawania i przetwarzania obrazów.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Filtry konwolucyjne\n",
    "\n",
    "Rozważmy sieć neuronową, w której wejściem są obrazy reprezentowane przez piksele. Zakładamy na początek, że mamy jeden kanał i obraz jest w skali szarości. Na obrazie tym ustawiane jest w różnych położeniach prostokątne **okno** (*receptive field*), które zwykle ma postać kwadratu o rozmiarze $3\\times 3$ lub $5\\times 5$. Wewnątrz okna znajdują się (zwykle znormalizowane) piksele obrazu, które są przetwarzane przez **filtr konwolucyjny** za pomocą operacji **korelacji krzyżowej** (*cross-correlation*):\n",
    "\n",
    "\\begin{equation*}\n",
    "z=\\mathbf{w}^T\\mathbf{x}+b,\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie $\\mathbf{w}$ to wektor wag filtra, $b$ to bias, a $\\mathbf{x}$ to wektor pikseli w oknie. Tablicę uzyskaną w wyniku zastosowania filtra $\\mathbf{K}$ do wszystkich okien w obrazie $\\mathbf{I}$ nazywamy **konwolucją** (*convolution*) i oznaczamy jako $\\mathbf{I}\\ast\\mathbf{K}$. \n",
    "\n",
    "Zatem dla filtra o rozmiarze $h\\times w$ mamy\n",
    "\n",
    "\\begin{equation*}\n",
    "[\\mathbf{I}\\ast\\mathbf{K}](i,j)=\\sum_{m=0}^{h-1}\\sum_{n=0}^{w-1} \\mathbf{I}(i+m,j+n)\\cdot \\mathbf{K}(m,n),\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie $(i,j)$ to indeksy wartości w tablicy $\\mathbf{I}\\ast\\mathbf{K}$.\n",
    "\n",
    "Przykład: Dla $\\mathbf{I}$ i $\\mathbf{K}$ zdefiniowanych jako\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{I}=\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{K}=\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "mamy\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{I}\\ast\\mathbf{K}=\\begin{bmatrix}\n",
    "1\\cdot 1+2\\cdot 2+4\\cdot 3+5\\cdot 4 & 2\\cdot 1+3\\cdot 2+5\\cdot 3+6\\cdot 4\\\\\n",
    "4\\cdot 1+5\\cdot 2+7\\cdot 3+8\\cdot 4 & 5\\cdot 1+6\\cdot 2+8\\cdot 3+9\\cdot 4\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "1+4+12+20 & 2+6+15+24\\\\\n",
    "4+10+21+32 & 5+12+24+36\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "37 & 47\\\\\n",
    "66 & 77\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "Na wyjściu z warstwy konwolucyjnej znajduje się funkcja aktywacji, najczęściej ReLU, która jest stosowana do każdego elementu tablicy $\\mathbf{I}\\ast\\mathbf{K}$. W ten sposób pojedyncze wyjście (neuron) uzyskuje postać:\n",
    "\n",
    "\\begin{equation*}\n",
    "z=\\text{ReLU}(\\mathbf{w}^T\\mathbf{x}+b).\n",
    "\\end{equation*}\n",
    "\n",
    "Neuron $z$ \"widzi\" tylko tę część obrazu, która znajduje się w oknie, ale wagi wszystkich neuronów w warstwie konwolucyjnej są współdzielone. Funkcja ReLU wygasza te korelacje krzyżowe, które są mniejsze od $-b$ i pozostawia bez zmiany te, które są większe. Ponieważ $\\mathbf{w}^T\\mathbf{x}$ jest zwykłym iloczynem skalarnym, więc będzie przyjmował największe wartości dla tych okien, w których piksele są podobne (bliskie współliniowości) do wag filtra. W ten sposób neuron $z$ \"wykrywa\" lokalne cechy obrazu, które są reprezentowane przez wagi filtra $\\mathbf{w}$.\n",
    "\n",
    "Wagi filtra $\\mathbf{w}$ są trenowane w procesie uczenia, a ich liczba jest znacznie mniejsza niż liczba wag w MLP. Na przykład, dla filtra o rozmiarze $3\\times 3$ mamy 9 wag i 1 bias. \n",
    "\n",
    "Poniżej na przykładzie pokazuję w jaki sposób filtry konwolucyjne rozpoznają cechy lokalne w obrazach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image # do przetwarzania obrazów rastrowych\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobieramy obrazek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Aptenodytes_forsteri_-Snow_Hill_Island%2C_Antarctica_-adults_and_juvenile-8.jpg/800px-Aptenodytes_forsteri_-Snow_Hill_Island%2C_Antarctica_-adults_and_juvenile-8.jpg\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "r = requests.get(url, headers=headers)\n",
    "r.raise_for_status()\n",
    "\n",
    "with open(\"penguin.jpg\", \"wb\") as f: # Zapisz obraz do pliku\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytujemy obrazek za pomocą `PIL.Image` i nieco zmniejszamy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"penguin.jpg\").convert(\"L\")\n",
    "img.thumbnail((img.height // 2, img.width // 2))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konwersja do tablicy NumPy: obrazek rastrowy w skali szarości to tablica 2D o wymiarach (wysokość, szerokość) i zakresie wartości od 0 do 255. Każda wartość odpowiada intensywności piksela w danym miejscu - od 0 (czarny) do 255 (biały). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(img) # Konwersja obrazu do tablicy NumPy\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalsza konwersja, tym razem do tensora PyTorch. Wartości są normalizowane do zakresu [0, 1] i dodawany jest wymiar kanału (1 dla skali szarości) i wymiar wsadu (1, ponieważ mamy tylko jeden obrazek). W ten sposób uzyskujemy tensor o wymiarach (1, 1, wysokość, szerokość). Kanały i wsad to kwestie techniczne wymagane przez PyTorch. Na ogół jest tak, że PyTorch przetwarza dane w postaci wsadów, a nie pojedynczych próbek i w przypadku obrazów mają one zwykle wiele kanałów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.astype(np.float32) / 255.0 # Normalizacja wartości pikseli do zakresu [0, 1]\n",
    "img_tensor = torch.tensor(arr, dtype=torch.float32) # Konwersja do tensora PyTorch\n",
    "img_tensor = img_tensor.unsqueeze(0).unsqueeze(0) # Dodanie wymiarów dla kanałów i wsadu\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiujemy filtr konwolucyjny do wykrywania krawędzi pionowych:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_filter = torch.tensor(\n",
    "    [[1, 0, -1],\n",
    "     [1, 0, -1],\n",
    "     [1, 0, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0) \n",
    "vertical_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonujemy konwolucję tensora obrazu z filtrem. Argument `padding=1` oznacza, że dodajemy jedną warstwę zer wokół obrazu, aby zachować jego rozmiar po konwolucji. Efektem operacji jest tensor o wymiarach (1, 1, wysokość, szerokość), który następnie przekształcamy do tablicy NumPy, normalizujemy z powrotem do zakresu [0, 255] i konwertujemy do formatu uint8. Na końcu przekształcamy tablicę NumPy na obrazek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_conv = nn.functional.conv2d(img_tensor, vertical_filter, padding=1) # Konwolucja z filtrem pionowym\n",
    "img_conv = img_conv.squeeze(0).squeeze(0).numpy() # Usunięcie wymiarów wsadu i kanałów\n",
    "img_conv = (img_conv - img_conv.min()) / (img_conv.max() - img_conv.min()) * 255 # Ponowna normalizacja do zakresu [0, 255]\n",
    "img_conv = img_conv.astype(np.uint8) # Konwersja do typu uint8\n",
    "img_conv = Image.fromarray(img_conv) # Konwersja do obrazu\n",
    "img_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenie 7.1** \n",
    "\n",
    "Zdefiniuj filtr konwolucyjny do wykrywania krawędzi poziomych i wykonaj konwolucję na tym samym obrazku. Porównaj wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Konwolucja obrazu o wymiarach $H\\times W$ z filtrem o wymiarach $h\\times w$ zmniejsza rozmiar obrazu zgodnie ze wzorem:\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{\\text{out}}=H_{\\text{in}}-h+1, \\quad W_{\\text{out}}=W_{\\text{in}}-w+1.\n",
    "\\end{equation*}\n",
    "\n",
    "*Padding*, czyli dopełnianie, to technika, która polega na dodaniu pikseli dookoła obrazu, aby uzyskać większy rozmiar wyjściowy. Dodanie $p$ pikseli dookoła obrazu o wymiarach $H\\times W$ daje obraz o wymiarach $(H+2p)\\times(W+2p)$. W przypadku konwolucji z filtrem o wymiarach $h\\times w$ dostajemy finalnie tablicę o wymiarach:\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{\\text{out}}=H_{\\text{in}}+2p-h+1, \\quad W_{\\text{out}}=W_{\\text{in}}+2p-w+1.\n",
    "\\end{equation*}\n",
    "\n",
    "Dodatkowe piksele wypełnia się zwykle zerami, o ile wcześniej od obrazu odjęto średnią, dzięki czemu 0 jest wartością średnią pikseli. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "\n",
    "*Stride* to krok, o jaki przesuwamy okno filtra po obrazie. Domyślnie jest równy 1, co oznacza, że filtr przesuwa się o jeden piksel w prawo lub w dół. Im większy krok, tym mniejszy rozmiar wyjściowy. Dla kroku $s$ i paddingu $p$ mamy:\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{\\text{out}}=\\left\\lfloor\\frac{H_{\\text{in}}+2p-h}{s}\\right\\rfloor-1, \\quad W_{\\text{out}}=\\left\\lfloor\\frac{W_{\\text{in}}+2p-w}{s}\\right\\rfloor-1.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwolucje z wieloma filtrami\n",
    "\n",
    "Dla obrazu o $C$ kanałach filtr konwolucyjny ma wymiary $h\\times w\\times C$ i posiada $hwC$ wag plus bias. Aby sieć mogła wykrywać różnorodne cechy lokalne, filtry w warstwie organizuje się w tensor o wymiarach $h\\times w\\times C\\times C_{\\text{out}}$, gdzie $C_{\\text{out}}$ to liczba filtrów w danej warstwie.\n",
    "\n",
    "Łączna liczba parametrów w warstwie konwolucyjnej wynosi $(hwC+1)\\cdot C_{\\text{out}}$. **Kluczowa zaleta**: liczba parametrów nie zależy od rozmiaru obrazu ($H$ i $W$) - parametrów jest znacznie mniej niż w analogicznym zadaniu wymagałaby sieć MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling to operacja nie podobna konwolucji, która zmniejsza rozmiar obrazu. I w tym przypadku stosuje się prostokątne okno (*receptive field*), które przesuwa się po obrazie, nie ma jednak żadnych parametrów do wytrenowania. Efektem poolingu na pojedynczym oknie jest wynik działania prostej funkcji agregującej na pikselach tego okna, np. maksimum lub średniej. Krok przesuwania okna jest zwykle taki sam jak rozmiar okna, co oznacza, że nie nakładają się one na siebie. Pooling stosowany jest do każdego kanału z osobna, zmniejsza więc rozmiar obrazu, ale nie zmienia liczby kanałów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equiwariantność i niezmienniczość\n",
    "\n",
    "Sieci konwolucyjne są equiwariantne względem przesunięcia (**translation equivariant**), co oznacza, że przesunięcie cech w obrazie skutkuje odpowiednim przesunięciem cech w wyjściu. Equiwariantność jest zapewniana przez współdzielenie wag w warstwie konwolucyjnej - cecha zostanie wykryta niezależnie od tego, gdzie się znajduje w obrazie, i zostanie uwidoczniona w odpowiednim miejscu w mapie aktywacji.\n",
    "\n",
    "Pooling powoduje, że lokalne niewielkie przesunięcia cech w obrazie nie mają wpływu na wyjście, ponieważ operacja ta wybiera dominujące wartości z małych regionów obrazu. W ten sposób sieci konwolucyjne stają się niezmiennicze (**translation invariant**) względem niewielkich przesunięć cech w obrazie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektura sieci konwolucyjnej\n",
    "\n",
    "Typowa architektura prostej sieci konwolucyjnej składa się z następujących po sobie warstw:\n",
    "1. konwolucyjna,\n",
    "2. aktywacji (np. ReLU),\n",
    "3. pooling (np. max pooling).\n",
    "\n",
    "Sieć kończy się zwykle warstwą wypłaszczającą (*flatten*), która przekształca mapy aktywacji w wektory, po której następuje w pełni połączona sieć neuronowa (MLP), która przetwarza aktywacje z ostatniej warstwy konwolucyjnej. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LeNet-5](http://en.wikipedia.org/wiki/LeNet-5)\n",
    "\n",
    "Jedna z pierwszych sieci konwolucyjnych, zaprojektowana przez Yann LeCun i jego współpracowników w 1998 roku. Została zaprojektowana do rozpoznawania cyfr w obrazach o rozmiarze $28\\times 28$ pikseli. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobieramy zbiór MNIST. Zawiera on 60 000 obrazków cyfr odręcznych w skali szarości o rozmiarze $28\\times 28$ pikseli w zbiorze uczącym i 10 000 analogicznych w zbiorze testowym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "mnist_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.1307], std=[0.3081]),\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=mnist_transform,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=mnist_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy loadery do zbiorów uczącego i testowego. Używamy `shuffle=True`, aby tasować dane w przed każdą kolejną epoką treningu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podgląd danych: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(training_data), 6)\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10, 5))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(training_data[idx[i]][0][0], cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Label: {training_data[idx[i]][1]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetMNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNetMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.activation = nn.Tanh()  \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x)) # --> 6x28x28\n",
    "        x = self.pool(x) # --> 6x14x14\n",
    "        x = self.activation(self.conv2(x)) # --> 16x10x10\n",
    "        x = self.pool(x) # --> 16x5x5\n",
    "        x = x.view(-1, 16 * 5 * 5) # Spłaszczenie do wektora\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNetMNIST() # Inicjalizacja modelu\n",
    "loss_fn = nn.CrossEntropyLoss() # Funkcja straty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # Optymalizator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # X: tensor o kształcie (batch_size, 1, 28, 28)\n",
    "        # y: tensor o kształcie (batch_size,)\n",
    "        # Predykcja i obliczenie straty na wsadzie\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Wykonanie kroku optymalizacji\n",
    "        optimizer.zero_grad() # Zerowanie gradientów\n",
    "        loss.backward() # Obliczanie gradientów\n",
    "        optimizer.step() # Aktualizacja wag\n",
    "        \n",
    "        # Wyświetlenie straty co 100 wsadów\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {loss.item():.4f}\")\n",
    "    print(\"Trening zakończony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przewidywania na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true, y_pred = [], [] # Inicjalizacja list do przechowywania prawdziwych i przewidywanych etykiet\n",
    "\n",
    "model.eval() # Ustawienie modelu w tryb ewaluacji\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        pred = model(X)\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Macierz pomyłek (Dokładność: {acc:.4f})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeksy obrazków, które zostały błędnie sklasyfikowane przez sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indeksy błędnych klasyfikacji\n",
    "wrong_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "wrong_indices[:10] # pierwsze 10 błędnych klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podgląd 6 losowo wybranych obrazków z błędnymi przewidywaniami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 losowych błędnych klasyfikacji\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10, 5))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    idx = np.random.choice(wrong_indices)\n",
    "    ax.imshow(test_data[idx][0][0], cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"True: {test_data[idx][1]}, Pred: {y_pred[idx]}\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
